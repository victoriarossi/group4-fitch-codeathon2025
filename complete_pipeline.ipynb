{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitch Codeathon 2025 - Complete Data Science Pipeline\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of our entire data analysis and modeling pipeline for predicting target_scope_1 and target_scope_2 emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Familiarization\n",
    "\n",
    "First, we analyze all datasets to understand their structure, content, and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data familiarization analysis\n",
    "subprocess.run(['python', 'data_familiarization.py'], check=True)\n",
    "print(\"✓ Data familiarization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Summary\n",
    "\n",
    "Let's examine the summary of our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary\n",
    "with open('dataset_summary.txt', 'r') as f:\n",
    "    summary = f.read()\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Data Familiarization:\n",
    "\n",
    "- **Environmental Activities Dataset**: Contains environmental score adjustments for different activity types\n",
    "- **Revenue Distribution Dataset**: Shows revenue breakdown by NACE sector codes\n",
    "- **Sustainable Development Goals Dataset**: Links entities to SDG commitments\n",
    "- **Training Dataset**: Main dataset with target variables (target_scope_1 and target_scope_2)\n",
    "\n",
    "The analysis revealed several highly skewed columns that will require transformation or outlier treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trend and Distribution Analysis\n",
    "\n",
    "Now we analyze the distributions of numeric columns and explore the relationship between target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trend and distribution analysis\n",
    "subprocess.run(['python', 'trend_n_distribution_analysis.py'], check=True)\n",
    "print(\"✓ Trend and distribution analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Visualizations\n",
    "\n",
    "### Numeric Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display distribution histograms\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "features = ['revenue', 'revenue_log', 'environmental_score', 'social_score', \n",
    "            'governance_score', 'target_scope_1', 'target_scope_2']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    if idx < 9:\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        img_path = f'plots/esg_distributions/{feature}_hist.png'\n",
    "        if os.path.exists(img_path):\n",
    "            img = plt.imread(img_path)\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis('off')\n",
    "            axes[row, col].set_title(feature)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(features), 9):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Scope Correlation Analysis\n",
    "\n",
    "We investigated the relationship between `target_scope_1` and `target_scope_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation analysis\n",
    "with open('trend_n_dist_analysis.txt', 'r') as f:\n",
    "    analysis = f.read()\n",
    "    # Extract correlation information\n",
    "    lines = analysis.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'correlation between' in line.lower():\n",
    "            print(line)\n",
    "            if i + 1 < len(lines):\n",
    "                print(lines[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scatter plots showing relationship between target variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "scatter_plots = [\n",
    "    ('plots/target_scope_1_vs_2_scatter.png', 'Target Scope 1 vs 2'),\n",
    "    ('plots/log_target_scope_1_vs_2_scatter.png', 'Log(Target Scope 1) vs 2'),\n",
    "    ('plots/log_target_scope_1_vs_2_lowess.png', 'LOWESS Smoothed Trend'),\n",
    "    ('plots/log_target_scope_1_vs_2_hexbin.png', 'Hexbin Density Plot')\n",
    "]\n",
    "\n",
    "for idx, (img_path, title) in enumerate(scatter_plots):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    if os.path.exists(img_path):\n",
    "        img = plt.imread(img_path)\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "        axes[row, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Discovery: Monotonic Relationship\n",
    "\n",
    "**Our analysis revealed that `target_scope_1` and `target_scope_2` have a MONOTONIC relationship:**\n",
    "\n",
    "- **Spearman correlation** (0.74+) is higher than **Pearson correlation** (0.65+)\n",
    "- This indicates that as `target_scope_1` increases, `target_scope_2` tends to increase as well\n",
    "- However, the relationship is **not linear** - it follows a monotonic but non-linear pattern\n",
    "- The LOWESS smoothing curve and hexbin density plot clearly show this non-linear trend\n",
    "\n",
    "**Implication for modeling**: We should use models that can capture non-linear relationships, such as tree-based methods or add polynomial/interaction features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outlier Treatment\n",
    "\n",
    "We apply outlier treatment to handle extreme values that could skew our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run outlier treatment\n",
    "subprocess.run(['python', 'outlier_treatment.py'], check=True)\n",
    "print(\"✓ Outlier treatment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Treatment Results\n",
    "\n",
    "### Treatment Strategy:\n",
    "\n",
    "1. **Revenue**: Applied log1p transformation (no capping needed)\n",
    "2. **Environmental Score Adjustments**: Capped at 1st and 99th percentiles for each activity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display outlier treatment summary\n",
    "with open('outlier_treatment_output.txt', 'r') as f:\n",
    "    treatment = f.read()\n",
    "    print(treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display before/after comparison for revenue\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plots = [\n",
    "    'plots/outlier_treatment/revenue/revenue_log_hist_before_after.png',\n",
    "    'plots/outlier_treatment/revenue/revenue_log_box_before_after.png'\n",
    "]\n",
    "\n",
    "for idx, img_path in enumerate(plots):\n",
    "    if os.path.exists(img_path):\n",
    "        img = plt.imread(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Revenue Outlier Treatment (Log Transformation)', fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Missing Values Strategy - Merging Datasets\n",
    "\n",
    "After outlier treatment, we merge all datasets to create a complete view and identify missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import merge function\n",
    "from merge_datasets import merge_after_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets after outlier treatment\n",
    "merge_after_outlier()\n",
    "print(\"✓ Datasets merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Missing Values Analysis\n",
    "\n",
    "Let's examine which values are missing in our merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged dataset\n",
    "merged_df = pd.read_csv('data/merged_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values by column\n",
    "print(\"\\n=== MISSING VALUES BY COLUMN ===\")\n",
    "print(\"=\"*50)\n",
    "missing_summary = merged_df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    pct = (count / len(merged_df)) * 100\n",
    "    print(f\"{col:40s}: {count:6d} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values by source dataset\n",
    "print(\"\\n=== MISSING VALUES BY SOURCE DATASET ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Environmental Activities columns\n",
    "env_cols = ['activity_type', 'activity_code', 'env_score_adjustment', 'env_score_adjustment_capped']\n",
    "env_missing = merged_df[env_cols].isnull().any(axis=1).sum()\n",
    "print(f\"\\nEnvironmental Activities: {env_missing} rows missing ({(env_missing/len(merged_df))*100:.2f}%)\")\n",
    "print(f\"  Columns: {', '.join(env_cols)}\")\n",
    "\n",
    "# Sustainable Development Goals columns\n",
    "sdg_cols = ['sdg_id', 'sdg_name']\n",
    "sdg_missing = merged_df[sdg_cols].isnull().any(axis=1).sum()\n",
    "print(f\"\\nSustainable Development Goals: {sdg_missing} rows missing ({(sdg_missing/len(merged_df))*100:.2f}%)\")\n",
    "print(f\"  Columns: {', '.join(sdg_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Imputation Strategy\n",
    "\n",
    "### Problem:\n",
    "We have significant missing values in two datasets:\n",
    "- **Environmental Activities** data (activity_type, activity_code, env_score_adjustment)\n",
    "- **Sustainable Development Goals** data (sdg_id, sdg_name)\n",
    "\n",
    "### Approaches Tested:\n",
    "We experimented with multiple imputation methods:\n",
    "1. **Random Forest**\n",
    "2. **Median/Mode**\n",
    "3. **Gradient Boosting**\n",
    "4. **K-Nearest Neighbors (KNN)**\n",
    "5. **MICE (Multiple Imputation by Chained Equations)**\n",
    "\n",
    "### Final Decision:\n",
    "After comparing model accuracy and confidence scores, we selected:\n",
    "\n",
    "- **Gradient Boosting** for Environmental Activities imputation\n",
    "  - Reason: Higher prediction confidence and better handling of complex feature interactions\n",
    "  - Cross-validation accuracy: ~75-80%\n",
    "  - Better feature importance interpretation\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)** for Sustainable Development Goals imputation\n",
    "  - Reason: Better confidence scores for categorical SDG predictions\n",
    "  - Cross-validation accuracy: ~70-75%\n",
    "  - More robust for multi-class classification with distance-based weighting\n",
    "\n",
    "Both methods showed superior performance compared to simpler approaches and provided high-confidence predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Running Imputations\n",
    "\n",
    "### 10.1 Gradient Boosting - Environmental Activities Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gradient Boosting imputation for environmental activities\n",
    "subprocess.run(['python', 'gb_env_imputation.py'], check=True)\n",
    "print(\"✓ Gradient Boosting imputation for environmental activities complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display GB imputation results\n",
    "with open('gb_env_imputation_log.txt', 'r') as f:\n",
    "    gb_log = f.read()\n",
    "    print(gb_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 KNN - Sustainable Development Goals Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN imputation for sustainable development goals\n",
    "subprocess.run(['python', 'knn_sdg_imputation.py'], check=True)\n",
    "print(\"✓ KNN imputation for sustainable development goals complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SDG imputation confidence analysis\n",
    "if os.path.exists('plots/sdg_confidence_analysis.png'):\n",
    "    img = plt.imread('plots/sdg_confidence_analysis.png')\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('SDG Imputation Confidence Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Merging Imputed Data\n",
    "\n",
    "Now we combine all imputed values into a single complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import merge function\n",
    "from merge_datasets import merge_after_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge imputed datasets into final complete dataset\n",
    "merge_after_nan()\n",
    "print(\"✓ Imputed datasets merged successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify completeness\n",
    "complete_df = pd.read_csv('data/merged_dataset_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FINAL DATASET COMPLETENESS ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total rows: {len(complete_df):,}\")\n",
    "print(f\"Total columns: {complete_df.shape[1]}\")\n",
    "print(f\"\\nMissing values remaining:\")\n",
    "remaining_missing = complete_df.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "if len(remaining_missing) > 0:\n",
    "    print(remaining_missing)\n",
    "else:\n",
    "    print(\"  ✓ No missing values in key columns!\")\n",
    "\n",
    "print(\"\\nDataset ready for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Engineering\n",
    "\n",
    "Feature engineering transforms raw data into model-ready features that capture complex relationships.\n",
    "\n",
    "### What it does:\n",
    "\n",
    "1. **Sector Revenue PCA**: \n",
    "   - Creates pivot table of revenue percentages by NACE sector codes\n",
    "   - Applies PCA to reduce dimensionality while preserving sector patterns\n",
    "   - Generates `Sector_Comp_1` through `Sector_Comp_10` features\n",
    "\n",
    "2. **Environmental Activity Aggregations**:\n",
    "   - Counts number of activities per entity (`num_activities`)\n",
    "   - Computes average environmental score adjustment\n",
    "   - Creates binary indicator for activity presence\n",
    "\n",
    "3. **Interaction Features**:\n",
    "   - `revenue_x_environmental_score`: Captures company size effect on environmental impact\n",
    "   - `revenue_x_governance_score`: Size-governance relationship\n",
    "   - `E_x_S`, `S_x_G`: ESG component interactions\n",
    "\n",
    "4. **Country-Level Features**:\n",
    "   - `country_ts2_per_revenue`: Average Scope 2 emissions per revenue for each country\n",
    "   - Helps capture regional regulatory and infrastructure differences\n",
    "\n",
    "5. **Log Transformations**:\n",
    "   - `target_scope_1_log` and `target_scope_2_log`\n",
    "   - Normalizes skewed target distributions for better model performance\n",
    "\n",
    "### Why this matters:\n",
    "\n",
    "- **Dimensionality Reduction**: PCA reduces hundreds of sector codes to 10 meaningful components\n",
    "- **Non-linear Patterns**: Interaction terms help models capture multiplicative relationships\n",
    "- **Geographic Context**: Country aggregations encode regional differences in emissions\n",
    "- **Better Model Performance**: Engineered features typically improve R² by 10-20% over raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature engineering\n",
    "subprocess.run(['python', 'feature_engineering.py'], check=True)\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features\n",
    "engineered_df = pd.read_csv('data/data_after_feature_extraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display engineered features summary\n",
    "print(\"\\n=== ENGINEERED FEATURES SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total features: {engineered_df.shape[1]}\")\n",
    "print(f\"\\nSample of new features:\")\n",
    "print(engineered_df[['entity_id', 'revenue_x_environmental_score', 'E_x_S', \n",
    "                      'num_activities', 'target_scope_1_log', 'target_scope_2_log']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PCA components\n",
    "print(\"\\nSector PCA Components:\")\n",
    "sector_comps = [col for col in engineered_df.columns if 'Sector_Comp_' in col]\n",
    "print(f\"  {len(sector_comps)} components created: {', '.join(sector_comps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Training\n",
    "\n",
    "We train and tune multiple models for predicting target_scope_1 and target_scope_2.\n",
    "\n",
    "### Process:\n",
    "1. **Baseline Models**: Random Forest, XGBoost, CatBoost, ElasticNet\n",
    "2. **Hyperparameter Tuning**: RandomizedSearchCV with 20 iterations\n",
    "3. **Model Selection**: Choose best model per target based on validation R²\n",
    "4. **Final Models Saved**:\n",
    "   - `best_scope1.joblib`: Best model for target_scope_1\n",
    "   - `best_scope2.joblib`: Best model for target_scope_2\n",
    "   - `feature_cols.joblib`: List of features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model training\n",
    "subprocess.run(['python', 'training_model.py'], check=True)\n",
    "print(\"✓ Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model training log\n",
    "with open('model_training_log.txt', 'r') as f:\n",
    "    training_log = f.read()\n",
    "    print(training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model metrics\n",
    "metrics_df = pd.read_csv('data/model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance for Scope 1\n",
    "print(\"\\n=== MODEL PERFORMANCE COMPARISON ===\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTarget Scope 1:\")\n",
    "scope1_metrics = metrics_df[metrics_df['target'] == 'target_scope_1_log'].sort_values('test_r2', ascending=False)\n",
    "print(scope1_metrics[['phase', 'model', 'val_r2', 'test_r2', 'test_mae', 'test_rmse']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance for Scope 2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTarget Scope 2:\")\n",
    "scope2_metrics = metrics_df[metrics_df['target'] == 'target_scope_2_log'].sort_values('test_r2', ascending=False)\n",
    "print(scope2_metrics[['phase', 'model', 'val_r2', 'test_r2', 'test_mae', 'test_rmse']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# R² comparison for Scope 1\n",
    "scope1_baseline = metrics_df[(metrics_df['target'] == 'target_scope_1_log') & \n",
    "                             (metrics_df['phase'] == 'baseline_phase10')]\n",
    "scope1_tuned = metrics_df[(metrics_df['target'] == 'target_scope_1_log') & \n",
    "                          (metrics_df['phase'] == 'tuned_scope1_phase10')]\n",
    "\n",
    "x = np.arange(len(scope1_baseline))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, scope1_baseline['test_r2'], width, label='Baseline', alpha=0.8)\n",
    "axes[0].bar(x + width/2, scope1_tuned['test_r2'], width, label='Tuned', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Test R²')\n",
    "axes[0].set_title('Target Scope 1: Baseline vs Tuned Models')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(scope1_baseline['model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R² comparison for Scope 2\n",
    "scope2_baseline = metrics_df[(metrics_df['target'] == 'target_scope_2_log') & \n",
    "                             (metrics_df['phase'] == 'baseline_phase10')]\n",
    "scope2_tuned = metrics_df[(metrics_df['target'] == 'target_scope_2_log') & \n",
    "                          (metrics_df['phase'] == 'tuned_scope2_phase10')]\n",
    "\n",
    "axes[1].bar(x - width/2, scope2_baseline['test_r2'], width, label='Baseline', alpha=0.8)\n",
    "axes[1].bar(x + width/2, scope2_tuned['test_r2'], width, label='Tuned', alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Test R²')\n",
    "axes[1].set_title('Target Scope 2: Baseline vs Tuned Models')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(scope2_baseline['model'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Processing Test Data\n",
    "\n",
    "To make predictions on test.csv, we must apply the SAME feature engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test data processing\n",
    "subprocess.run(['python', 'process_test_data.py'], check=True)\n",
    "print(\"✓ Test data feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Why Test Data Processing is Critical\n",
    "\n",
    "### The Problem:\n",
    "Our models were trained on **engineered features**, not raw data. The test.csv file only contains raw features.\n",
    "\n",
    "### What process_test_data.py does:\n",
    "\n",
    "1. **Merges supplementary datasets**: Joins environmental_activities, SDG, and revenue data to test entities\n",
    "\n",
    "2. **Recreates PCA components**: \n",
    "   - Uses training data to fit StandardScaler and PCA\n",
    "   - Transforms test revenue distribution into same `Sector_Comp_1-10` space\n",
    "   - Critical: Must use SAME scaler/PCA fitted on training data\n",
    "\n",
    "3. **Generates aggregations**:\n",
    "   - Environmental activity counts and averages\n",
    "   - Sector-level statistics (from training data)\n",
    "   - Country-level statistics (from training data)\n",
    "\n",
    "4. **Creates interaction features**:\n",
    "   - All multiplication/ratio features: `revenue_x_environmental_score`, `E_x_S`, etc.\n",
    "   - ESG PCA components using training data's PCA transformation\n",
    "\n",
    "5. **Handles missing values**:\n",
    "   - Fills with training data medians to maintain consistency\n",
    "   - Ensures no NaN values that would break prediction\n",
    "\n",
    "### Why this matters:\n",
    "\n",
    "**Feature alignment is crucial**:\n",
    "- Models expect EXACT same features in SAME order\n",
    "- PCA/scaling must use training parameters (not refit on test)\n",
    "- Aggregations must reference training statistics\n",
    "\n",
    "**Without proper feature engineering**:\n",
    "- ❌ Model would see completely different feature space\n",
    "- ❌ Predictions would be meaningless or fail entirely\n",
    "- ❌ Feature names/counts wouldn't match\n",
    "\n",
    "**With proper feature engineering**:\n",
    "- ✓ Test data in same feature space as training\n",
    "- ✓ Models can make accurate predictions\n",
    "- ✓ Results are comparable and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and feature list\n",
    "test_engineered = pd.read_csv('data/test_after_feature_engineering.csv')\n",
    "feature_cols = joblib.load('models/feature_cols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify feature alignment\n",
    "print(\"\\n=== FEATURE ALIGNMENT CHECK ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training features expected: {len(feature_cols)}\")\n",
    "print(f\"Test features available: {len([c for c in feature_cols if c in test_engineered.columns])}\")\n",
    "\n",
    "missing_features = [col for col in feature_cols if col not in test_engineered.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\n⚠️  Missing features: {missing_features}\")\n",
    "else:\n",
    "    print(\"\\n✓ All required features present in test data\")\n",
    "\n",
    "print(f\"\\nTest data shape: {test_engineered.shape}\")\n",
    "print(f\"Test entities: {test_engineered['entity_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Generate Predictions\n",
    "\n",
    "Now we use our trained models to predict target_scope_1 and target_scope_2 for test entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "subprocess.run(['python', 'predict_both_scopes.py'], check=True)\n",
    "print(\"✓ Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Final Predictions\n",
    "\n",
    "Let's examine our predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "predictions = pd.read_csv('data/test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n=== PREDICTIONS SUMMARY ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(predictions[['pred_target_scope_1', 'pred_target_scope_2']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "print(predictions.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Scope 1 histogram\n",
    "axes[0, 0].hist(predictions['pred_target_scope_1'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Predicted Target Scope 1')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Predicted Target Scope 1')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scope 2 histogram\n",
    "axes[0, 1].hist(predictions['pred_target_scope_2'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Predicted Target Scope 2')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Predicted Target Scope 2')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scope 1 vs Scope 2 scatter\n",
    "axes[1, 0].scatter(predictions['pred_target_scope_1'], predictions['pred_target_scope_2'], \n",
    "                   alpha=0.5, s=20)\n",
    "axes[1, 0].set_xlabel('Predicted Target Scope 1')\n",
    "axes[1, 0].set_ylabel('Predicted Target Scope 2')\n",
    "axes[1, 0].set_title('Predicted Scope 1 vs Scope 2 Relationship')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Log scale scatter\n",
    "axes[1, 1].scatter(np.log1p(predictions['pred_target_scope_1']), \n",
    "                   np.log1p(predictions['pred_target_scope_2']), \n",
    "                   alpha=0.5, s=20, color='green')\n",
    "axes[1, 1].set_xlabel('Log(Predicted Target Scope 1 + 1)')\n",
    "axes[1, 1].set_ylabel('Log(Predicted Target Scope 2 + 1)')\n",
    "axes[1, 1].set_title('Log-Scale: Predicted Scope 1 vs Scope 2')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation in predictions\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "pearson_corr, _ = pearsonr(predictions['pred_target_scope_1'], predictions['pred_target_scope_2'])\n",
    "spearman_corr, _ = spearmanr(predictions['pred_target_scope_1'], predictions['pred_target_scope_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== PREDICTED CORRELATION ANALYSIS ===\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Pearson correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f}\")\n",
    "print(\"\\nNote: Our predictions maintain the monotonic relationship observed in training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Complete Pipeline:\n",
    "\n",
    "1. ✅ **Data Familiarization**: Analyzed 4 datasets, identified skewness and missing values\n",
    "2. ✅ **Distribution Analysis**: Discovered monotonic (non-linear) relationship between target scopes\n",
    "3. ✅ **Outlier Treatment**: Applied log transformation and percentile capping\n",
    "4. ✅ **Imputation**: \n",
    "   - Gradient Boosting for Environmental Activities (~75-80% accuracy)\n",
    "   - KNN for Sustainable Development Goals (~70-75% accuracy)\n",
    "5. ✅ **Feature Engineering**: Created 50+ engineered features including PCA, interactions, aggregations\n",
    "6. ✅ **Model Training**: Tuned 4 algorithms, selected best for each target\n",
    "7. ✅ **Test Processing**: Applied identical feature engineering to test data\n",
    "8. ✅ **Predictions**: Generated emissions predictions maintaining observed relationships\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Non-linear relationships** between emission scopes required sophisticated models\n",
    "- **Feature engineering** was crucial - models perform significantly better with engineered features\n",
    "- **Careful imputation** maintained data quality while handling 30-40% missing values\n",
    "- **Test data processing** required exact replication of training transformations\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `data/test_predictions.csv` - Final predictions\n",
    "- `models/best_scope1.joblib` - Trained model for Scope 1\n",
    "- `models/best_scope2.joblib` - Trained model for Scope 2\n",
    "- `models/feature_cols.joblib` - Feature list for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}